{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDuDNvw2YMUD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade diffusers[torch]\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I30SCM7FYMQ2"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0xAiiQEYMNo"
      },
      "outputs": [],
      "source": [
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XmS_5JmYTG_"
      },
      "outputs": [],
      "source": [
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def get_clip_score(image, prompt):\n",
        "    inputs = clip_processor(\n",
        "        text=[prompt[:75]],\n",
        "        images=image,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    score = logits_per_image.softmax(dim=1)[0][0].item()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqIjZiEyYTDP"
      },
      "outputs": [],
      "source": [
        "genders = ['male', 'female']\n",
        "\n",
        "emotion_prompts = {\n",
        "    'happy': 'smiling, joyful, delighted',\n",
        "    'sad': 'frowning, sad face expression, crying',\n",
        "    'angry': 'angry, hostility',\n",
        "    'surprised': 'surprised, opened mouth, raised eyebrows'\n",
        "}\n",
        "\n",
        "with open('ethnicities.json', 'r') as f:\n",
        "    ethnicities = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QzkzGwKYZFp"
      },
      "outputs": [],
      "source": [
        "positive_prompt = 'Medium-shot portrait of {} {}, {}, human, front view, looking at the camera, sharp clear detailed expressive eyes, perfect face, symmetrical face, color photography, photorealistic, hyperrealistic, realistic, incredibly detailed, crisp focus, digital art, depth of field, 50mm, 8k'\n",
        "negative_prompt = '3d, cartoon, anime, sketches, (worst quality:2), (low quality:2), (normal quality:2), lowres, normal quality, ((monochrome)), ((grayscale)), Low Quality, Worst Quality, plastic, fake, disfigured, fused eyes, mutated face, deformed, blurry, bad anatomy, blurred, watermark, grainy, signature'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnPWp_bhLJ2p"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "for j in range(3):\n",
        "    for emotion, emotion_prompt in emotion_prompts.items():\n",
        "        ethnicity = random.choice(ethnicities)\n",
        "        gender = random.choice(genders)\n",
        "        prompt = positive_prompt.format(ethnicity, gender, emotion_prompt)\n",
        "\n",
        "        best_img, best_score = None, -1\n",
        "        for _ in range(3):\n",
        "            image = pipeline(prompt, negative_prompt=negative_prompt).images[0]\n",
        "            score = get_clip_score(image, prompt)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_img = image\n",
        "\n",
        "        save_path = os.path.join(BASE_DIR, emotion, f\"{str(j).zfill(4)}.png\")\n",
        "        best_img.save(save_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObRkvTjQYixN"
      },
      "outputs": [],
      "source": [
        "shutil.make_archive(\"faces_dataset\", 'zip', BASE_DIR)\n",
        "print(\"All images saved and zipped as 'faces_dataset.zip'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
